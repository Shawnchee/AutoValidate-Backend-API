{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6521f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset \n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.append(notebook_dir)\n",
    "\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "OUTPUT_PATH = \"./model/finetuned-embedding-model\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47da554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL')\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce753c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a7054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "collection_name = \"car_data_modelbrand\"  \n",
    "\n",
    "# Function to search car brands with typo tolerance\n",
    "def search_car_brand(query, top_k=10):\n",
    "    # Add context to the query to match our embeddings\n",
    "    query_with_context = f\"car brand: {query}\" \n",
    "    query_vector = model.encode(query_with_context)\n",
    "    \n",
    "    # Use query_filter instead of filter parameter\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k * 3,\n",
    "        query_filter={\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"key\": \"vector_type\",\n",
    "                    \"match\": {\n",
    "                        \"value\": \"brand\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Remove duplicates based on car_brand\n",
    "    unique_results_brand = {}\n",
    "    for result in search_result:\n",
    "        car_model = result.payload['car_brand']\n",
    "        if car_model not in unique_results_brand:\n",
    "            unique_results_brand[car_model] = result\n",
    "    \n",
    "    # Return top_k unique results\n",
    "    return list(unique_results_brand.values())[:top_k]\n",
    "\n",
    "\n",
    "# Function to search car models with typo tolerance\n",
    "def search_car_model(query, top_k=10):\n",
    "    # Add generic context to the query to match our embeddings\n",
    "    query_with_context = f\"car model: {query}\"  \n",
    "    query_vector = model.encode(query_with_context)\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k * 3,\n",
    "        query_filter={\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"key\": \"vector_type\",\n",
    "                    \"match\": {\n",
    "                        \"value\": \"model\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Remove duplicates based on car_model\n",
    "    unique_results_model = {}\n",
    "    for result in search_result:\n",
    "        car_model = result.payload['car_model']\n",
    "        if car_model not in unique_results_model:\n",
    "            unique_results_model[car_model] = result\n",
    "    \n",
    "    # Return top_k unique results\n",
    "    return list(unique_results_model.values())[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41a77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process as rapidfuzz_process\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text to improve matching\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r'[\\-–—_/]', ' ', s)  \n",
    "    s = re.sub(r'\\s+', ' ', s)     \n",
    "    return s  \n",
    "\n",
    "def hybrid_search(query, choices, vector_type=\"brand\", fuzzy_threshold=75, top_k=3, search_model = None):\n",
    "    \"\"\"\n",
    "    Hybrid search that combines RapidFuzz and embeddings:\n",
    "    1. Try fuzzy matching first with 75% threshold (fast + handles typos)\n",
    "    2. If no good fuzzy matches, fall back to embeddings (semantic understanding)\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        choices: List of choices to search against\n",
    "        vector_type: \"brand\" or \"model\"\n",
    "        fuzzy_threshold: Minimum score (0-100) for fuzzy matches\n",
    "        top_k: Number of results to return\n",
    "    \"\"\"\n",
    "    # Normalize query for better matching\n",
    "    query_norm = normalize_text(query)\n",
    "\n",
    "    model_to_use = search_model if search_model is not None else model\n",
    "\n",
    "    \n",
    "    # Adjust scorer based on query characteristics\n",
    "    if ' ' in query_norm or len(query_norm) > 10:\n",
    "        scorer = fuzz.token_sort_ratio  # Better for word order/spacing differences\n",
    "    else:\n",
    "        scorer = fuzz.ratio  # Standard for character-level typos\n",
    "    \n",
    "    # Step 1: Try RapidFuzz first (faster than embeddings)\n",
    "    fuzzy_matches = rapidfuzz_process.extract(\n",
    "        query_norm, \n",
    "        [normalize_text(c) for c in choices],  # Normalize choices too\n",
    "        scorer=scorer,\n",
    "        limit=top_k * 2  # Get more candidates for filtering\n",
    "    )\n",
    "    \n",
    "    # Map normalized choices back to original labels\n",
    "    norm_to_orig = {normalize_text(c): c for c in choices}\n",
    "    fuzzy_matches = [(norm_to_orig.get(match, match), score, idx) for match, score, idx in fuzzy_matches]\n",
    "    \n",
    "    # Filter matches that meet our threshold\n",
    "    good_fuzzy_matches = [(match, score) for match, score, _ in fuzzy_matches if score >= fuzzy_threshold]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # If we have good fuzzy matches, return those\n",
    "    if good_fuzzy_matches:\n",
    "        print(f\"Found {len(good_fuzzy_matches)} good fuzzy matches for '{query}'\")\n",
    "        for match, score in good_fuzzy_matches:\n",
    "            results.append({\n",
    "                \"text\": match,\n",
    "                \"score\": score / 100.0,  # Normalize to 0-1 scale\n",
    "                \"source\": \"fuzzy\"\n",
    "            })\n",
    "    \n",
    "    # Step 2: If not enough good fuzzy matches, use embeddings\n",
    "    if len(results) == 0:\n",
    "        print(f\"No good fuzzy matches above threshold {fuzzy_threshold}, using embeddings\")\n",
    "        \n",
    "        # Use appropriate search function based on vector_type\n",
    "        if vector_type == \"brand\":\n",
    "            # Modify how you call search_car_brand to use the provided model\n",
    "            query_with_context = f\"car brand: {query}\"\n",
    "            query_vector = model_to_use.encode(query_with_context)\n",
    "            \n",
    "            # Call Qdrant directly with the new embedding\n",
    "            search_result = qdrant_client.search(\n",
    "                collection_name=collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k * 3,\n",
    "                query_filter={\"must\": [{\"key\": \"vector_type\", \"match\": {\"value\": \"brand\"}}]}\n",
    "            )\n",
    "            \n",
    "            # Process results as in search_car_brand\n",
    "            embedding_results = []\n",
    "            unique_results = {}\n",
    "            for result in search_result:\n",
    "                car_brand = result.payload['car_brand']\n",
    "                if car_brand not in unique_results:\n",
    "                    unique_results[car_brand] = result\n",
    "                    embedding_results.append(result)\n",
    "        else:  # model\n",
    "            # Similar for model search\n",
    "            query_with_context = f\"car model: {query}\"\n",
    "            query_vector = model_to_use.encode(query_with_context)\n",
    "            \n",
    "            # Call Qdrant directly\n",
    "            search_result = qdrant_client.search(\n",
    "                collection_name=collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k * 3,\n",
    "                query_filter={\"must\": [{\"key\": \"vector_type\", \"match\": {\"value\": \"model\"}}]}\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            embedding_results = []\n",
    "            unique_results = {}\n",
    "            for result in search_result:\n",
    "                car_model = result.payload['car_model']\n",
    "                if car_model not in unique_results:\n",
    "                    unique_results[car_model] = result\n",
    "                    embedding_results.append(result)\n",
    "        \n",
    "        # Extract relevant information\n",
    "        for result in embedding_results:\n",
    "            if vector_type == \"brand\":\n",
    "                text = result.payload.get(\"car_brand\")\n",
    "            else:  # model\n",
    "                text = result.payload.get(\"car_model\")\n",
    "                \n",
    "            # Skip if this result is already in our list from fuzzy matching\n",
    "            if any(r[\"text\"] == text for r in results):\n",
    "                continue\n",
    "                \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"score\": result.score,\n",
    "                \"source\": \"embedding\"\n",
    "            })\n",
    "    \n",
    "    # Return top_k results, sorted by score\n",
    "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079951bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correction</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neesun</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>benz</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>merz</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mercedesbenz</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toyata</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>toyta</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hunda</td>\n",
       "      <td>Honda</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hoonda</td>\n",
       "      <td>Honda</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>protan</td>\n",
       "      <td>Proton</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>perodwa</td>\n",
       "      <td>Perodua</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>axla</td>\n",
       "      <td>Axia</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xseventy</td>\n",
       "      <td>X70</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vios</td>\n",
       "      <td>Vios</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vios</td>\n",
       "      <td>Vios</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sivic</td>\n",
       "      <td>Civic</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>civek</td>\n",
       "      <td>Civic</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cityy</td>\n",
       "      <td>City</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>x fivty</td>\n",
       "      <td>X50</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>exora</td>\n",
       "      <td>Exora</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           query     correction domain\n",
       "0         neesun         Nissan  brand\n",
       "1           benz  Mercedes-Benz  brand\n",
       "2           merz  Mercedes-Benz  brand\n",
       "3   mercedesbenz  Mercedes-Benz  brand\n",
       "4         toyata         Toyota  brand\n",
       "5          toyta         Toyota  brand\n",
       "6          hunda          Honda  brand\n",
       "7         hoonda          Honda  brand\n",
       "8         protan         Proton  brand\n",
       "9        perodwa        Perodua  brand\n",
       "10          axla           Axia  model\n",
       "11      xseventy            X70  model\n",
       "12          vios           Vios  model\n",
       "13          vios           Vios  model\n",
       "14         sivic          Civic  model\n",
       "15         civek          Civic  model\n",
       "16         cityy           City  model\n",
       "17       x fivty            X50  model\n",
       "18         exora          Exora  model"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Collect training pairs from your examples\n",
    "\n",
    "# Brand test cases\n",
    "brand_typo_pairs = [\n",
    "    (\"neesun\", \"Nissan\"),\n",
    "    (\"benz\", \"Mercedes-Benz\"),\n",
    "    (\"merz\", \"Mercedes-Benz\"),\n",
    "    (\"mercedesbenz\", \"Mercedes-Benz\"),\n",
    "    (\"toyata\", \"Toyota\"),\n",
    "    (\"toyta\", \"Toyota\"),\n",
    "    (\"hunda\", \"Honda\"),\n",
    "    (\"hoonda\", \"Honda\"),\n",
    "    (\"protan\", \"Proton\"),\n",
    "    (\"perodwa\", \"Perodua\"),\n",
    "]\n",
    "\n",
    "# Model test cases - expanded\n",
    "model_typo_pairs = [\n",
    "    (\"axla\", \"Axia\"),\n",
    "    (\"xseventy\", \"X70\"),\n",
    "    (\"vios\", \"Vios\"),\n",
    "    (\"vios\", \"Vios\"),\n",
    "    (\"sivic\", \"Civic\"),\n",
    "    (\"civek\", \"Civic\"),\n",
    "    (\"cityy\", \"City\"),\n",
    "    (\"x fivty\", \"X50\"),\n",
    "    (\"exora\", \"Exora\"),\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame([\n",
    "    {\"query\": query, \"correction\": correction, \"domain\": \"brand\"} \n",
    "    for query, correction in brand_typo_pairs\n",
    "] + [\n",
    "    {\"query\": query, \"correction\": correction, \"domain\": \"model\"} \n",
    "    for query, correction in model_typo_pairs\n",
    "])\n",
    "\n",
    "print(f\"Total training pairs: {len(train_df)}\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5284d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_examples\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create training examples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m train_examples = prepare_training_examples(\u001b[43mtrain_df\u001b[49m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_examples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training examples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Create sentence-transformers training examples\n",
    "\n",
    "\n",
    "\n",
    "# Create training examples\n",
    "train_examples = prepare_training_examples(train_df)\n",
    "print(f\"Created {len(train_examples)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a5951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING BRAND TYPO DETECTION\n",
      "\n",
      "No good fuzzy matches above threshold 75, using embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aykay\\AppData\\Local\\Temp\\ipykernel_22416\\3121691710.py:77: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'neesun' (expected: Nissan)\n",
      "  1. Perodua (0.9028, embedding)  \n",
      "  2. Nissan (0.9001, embedding) ✓\n",
      "  3. Proton (0.8953, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'benz' (expected: Mercedes-Benz)\n",
      "  1. Perodua (0.9207, embedding)  \n",
      "  2. Mercedes-Benz (0.9107, embedding) ✓\n",
      "  3. Toyota (0.9030, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'merz' (expected: Mercedes-Benz)\n",
      "  1. Chery (0.9141, embedding)  \n",
      "  2. Perodua (0.9092, embedding)  \n",
      "  3. Mercedes-Benz (0.8999, embedding) ✓\n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'mercedesbenz' (expected: Mercedes-Benz)\n",
      "  1. Perodua (0.9236, embedding)  \n",
      "  2. Mercedes-Benz (0.9232, embedding) ✓\n",
      "  3. Chery (0.9030, embedding)  \n",
      "------------------------------------------------------------\n",
      "\n",
      "=== BRAND TYPO DETECTION SUMMARY ===\n",
      "Total cases: 4\n",
      "Resolved by fuzzy: 0 (0.0%)\n",
      "Resolved by embeddings: 4 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# 5. Load car data for testing\n",
    "df = pd.read_csv('car_dataset.csv')\n",
    "brand_choices = list(df['car_brand'].unique())\n",
    "model_choices = list(df['car_model'].unique())\n",
    "\n",
    "# Create test cases based on your examples\n",
    "brand_eval_cases = [\n",
    "    (\"neesun\", \"Nissan\"),\n",
    "    (\"benz\", \"Mercedes-Benz\"),\n",
    "    (\"merz\", \"Mercedes-Benz\"),\n",
    "    (\"mercedesbenz\", \"Mercedes-Benz\"),\n",
    "]\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(\"\\nTESTING BRAND TYPO DETECTION\\n\")\n",
    "\n",
    "# Track stats\n",
    "brand_stats = {\"total\": len(brand_eval_cases), \"fuzzy\": 0, \"embedding\": 0}\n",
    "\n",
    "for query, expected in brand_eval_cases:\n",
    "    # Test brand search\n",
    "    results = hybrid_search(query, brand_choices, vector_type=\"brand\", fuzzy_threshold=75, top_k=3, search_model=model)\n",
    "    \n",
    "    # Track which method provided the results\n",
    "    if results and results[0][\"source\"] == \"fuzzy\":\n",
    "        brand_stats[\"fuzzy\"] += 1\n",
    "    elif results:\n",
    "        brand_stats[\"embedding\"] += 1\n",
    "        \n",
    "    # Print results\n",
    "    print(f\"\\nQuery: '{query}' (expected: {expected})\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        match = \"✓\" if res[\"text\"] == expected else \" \"\n",
    "        print(f\"  {i}. {res['text']} ({res['score']:.4f}, {res['source']}) {match}\")\n",
    "    \n",
    "    # Visual separator\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\n=== BRAND TYPO DETECTION SUMMARY ===\")\n",
    "print(f\"Total cases: {brand_stats['total']}\")\n",
    "print(f\"Resolved by fuzzy: {brand_stats['fuzzy']} ({brand_stats['fuzzy']/brand_stats['total']*100:.1f}%)\")\n",
    "print(f\"Resolved by embeddings: {brand_stats['embedding']} ({brand_stats['embedding']/brand_stats['total']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINE-TUNING THE MODEL\n",
      "\n",
      "Training on: cpu\n",
      "Starting fine-tuning for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to ./model/finetuned-embedding-model\n"
     ]
    }
   ],
   "source": [
    "# 6. Fine-tune the model\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "print(\"\\nFINE-TUNING THE MODEL\\n\")\n",
    "\n",
    "# Load base model for fine-tuning\n",
    "fine_tune_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Create data loader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define loss function - we use contrastive loss to make similar pairs closer\n",
    "train_loss = losses.MultipleNegativesRankingLoss(fine_tune_model)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Fine-tune\n",
    "print(f\"Starting fine-tuning for {EPOCHS} epochs...\")\n",
    "warmup_steps = int(len(train_dataloader) * EPOCHS * 0.1)  # 10% of training as warmup\n",
    "\n",
    "fine_tune_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Model fine-tuned and saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b4475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING FINE-TUNED MODEL\n",
      "\n",
      "\n",
      "FINE-TUNED MODEL RESULTS\n",
      "\n",
      "No good fuzzy matches above threshold 75, using embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aykay\\AppData\\Local\\Temp\\ipykernel_22416\\3121691710.py:77: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'neesun' (expected: Nissan)\n",
      "  1. Nissan (0.6039, embedding) ✓\n",
      "  2. Toyota (0.4988, embedding)  \n",
      "  3. Mitsubishi (0.4841, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'benz' (expected: Mercedes-Benz)\n",
      "  1. Mercedes-Benz (0.5434, embedding) ✓\n",
      "  2. Perodua (0.5218, embedding)  \n",
      "  3. BMW (0.4875, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'merz' (expected: Mercedes-Benz)\n",
      "  1. Mercedes-Benz (0.5536, embedding) ✓\n",
      "  2. Chery (0.5491, embedding)  \n",
      "  3. Perodua (0.5171, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'mercedesbenz' (expected: Mercedes-Benz)\n",
      "  1. Mercedes-Benz (0.6245, embedding) ✓\n",
      "  2. Perodua (0.5866, embedding)  \n",
      "  3. Chery (0.5607, embedding)  \n",
      "------------------------------------------------------------\n",
      "Found 2 good fuzzy matches for 'axla'\n",
      "\n",
      "Query: 'axla' (expected: Axia)\n",
      "  1. Alza (0.7500, fuzzy)  \n",
      "  2. Axia (0.7500, fuzzy) ✓\n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'xseventy' (expected: X70)\n",
      "  1. X70 (0.6537, embedding) ✓\n",
      "  2. X90 (0.6190, embedding)  \n",
      "  3. CX-8 (0.6161, embedding)  \n",
      "------------------------------------------------------------\n",
      "\n",
      "=== FINE-TUNED MODEL DETECTION SUMMARY ===\n",
      "Total test cases: 6\n",
      "\n",
      "Brand cases: 4\n",
      "Resolved by fuzzy: 0 (0.0%)\n",
      "Resolved by embeddings: 4 (100.0%)\n",
      "\n",
      "Model cases: 2\n",
      "Resolved by fuzzy: 1 (50.0%)\n",
      "Resolved by embeddings: 1 (50.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aykay\\AppData\\Local\\Temp\\ipykernel_22416\\3121691710.py:98: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "# 7. Test with fine-tuned model\n",
    "\n",
    "print(\"\\nTESTING FINE-TUNED MODEL\\n\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "finetuned_model = SentenceTransformer(OUTPUT_PATH)\n",
    "\n",
    "# Create test cases based on your examples - include both brand and model\n",
    "test_cases = [\n",
    "    # Brand test cases\n",
    "    (\"neesun\", \"Nissan\", \"brand\"),\n",
    "    (\"benz\", \"Mercedes-Benz\", \"brand\"),\n",
    "    (\"merz\", \"Mercedes-Benz\", \"brand\"),\n",
    "    (\"mercedesbenz\", \"Mercedes-Benz\", \"brand\"),\n",
    "    # Model test cases\n",
    "    (\"axla\", \"Axia\", \"model\"),\n",
    "    (\"xseventy\", \"X70\", \"model\"),\n",
    "]\n",
    "\n",
    "# Track stats\n",
    "finetuned_stats = {\"total\": 0, \"brand\": {\"fuzzy\": 0, \"embedding\": 0}, \"model\": {\"fuzzy\": 0, \"embedding\": 0}}\n",
    "\n",
    "# Count total cases by type\n",
    "finetuned_stats[\"brand_total\"] = sum(1 for _, _, domain in test_cases if domain == \"brand\")\n",
    "finetuned_stats[\"model_total\"] = sum(1 for _, _, domain in test_cases if domain == \"model\")\n",
    "finetuned_stats[\"total\"] = len(test_cases)\n",
    "\n",
    "print(\"\\nFINE-TUNED MODEL RESULTS\\n\")\n",
    "\n",
    "for query, expected, domain in test_cases:\n",
    "    # Set choices based on domain\n",
    "    choices = brand_choices if domain == \"brand\" else model_choices\n",
    "    \n",
    "    # Test search with fine-tuned model\n",
    "    results = hybrid_search(\n",
    "        query, choices, vector_type=domain, \n",
    "        fuzzy_threshold=75, top_k=3, search_model=finetuned_model\n",
    "    )\n",
    "    \n",
    "    # Track which method provided the results\n",
    "    if results and results[0][\"source\"] == \"fuzzy\":\n",
    "        finetuned_stats[domain][\"fuzzy\"] += 1\n",
    "    elif results:\n",
    "        finetuned_stats[domain][\"embedding\"] += 1\n",
    "        \n",
    "    # Print results\n",
    "    print(f\"\\nQuery: '{query}' (expected: {expected})\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        match = \"✓\" if res[\"text\"] == expected else \" \"\n",
    "        print(f\"  {i}. {res['text']} ({res['score']:.4f}, {res['source']}) {match}\")\n",
    "    \n",
    "    # Visual separator\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\n=== FINE-TUNED MODEL DETECTION SUMMARY ===\")\n",
    "print(f\"Total test cases: {finetuned_stats['total']}\")\n",
    "\n",
    "print(f\"\\nBrand cases: {finetuned_stats['brand_total']}\")\n",
    "if finetuned_stats['brand_total'] > 0:\n",
    "    print(f\"Resolved by fuzzy: {finetuned_stats['brand']['fuzzy']} ({finetuned_stats['brand']['fuzzy']/finetuned_stats['brand_total']*100:.1f}%)\")\n",
    "    print(f\"Resolved by embeddings: {finetuned_stats['brand']['embedding']} ({finetuned_stats['brand']['embedding']/finetuned_stats['brand_total']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nModel cases: {finetuned_stats['model_total']}\")\n",
    "if finetuned_stats['model_total'] > 0:\n",
    "    print(f\"Resolved by fuzzy: {finetuned_stats['model']['fuzzy']} ({finetuned_stats['model']['fuzzy']/finetuned_stats['model_total']*100:.1f}%)\")\n",
    "    print(f\"Resolved by embeddings: {finetuned_stats['model']['embedding']} ({finetuned_stats['model']['embedding']/finetuned_stats['model_total']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e8e74",
   "metadata": {},
   "source": [
    "## Retrain the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6731b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from supabase import create_client\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def zip_model(model_path, output_zip=None):\n",
    "    \"\"\"\n",
    "    Zip a model directory into a compressed file\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model directory\n",
    "        output_zip: Path for the output zip file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created zip file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
    "    \n",
    "    # Create a zip filename with current date if not provided\n",
    "    if output_zip is None:\n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        output_zip = f\"model_{today}.zip\"\n",
    "    \n",
    "    # Create the zip file\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Walk through all files in the directory\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                # Calculate path for file in zip\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(model_path))\n",
    "                \n",
    "                # Add file to zip\n",
    "                zipf.write(file_path, arcname=arcname)\n",
    "    \n",
    "    print(f\"Model zipped successfully to {output_zip}\")\n",
    "    return output_zip\n",
    "\n",
    "def upload_model_to_supabase(zip_path, supabase_client, bucket_name=\"codenection-sss-model-training\"):\n",
    "    \"\"\"\n",
    "    Upload a zipped model to Supabase storage\n",
    "    \n",
    "    Args:\n",
    "        zip_path: Path to the zipped model file\n",
    "        supabase_client: Initialized Supabase client\n",
    "        bucket_name: Name of the storage bucket\n",
    "    \n",
    "    Returns:\n",
    "        URL of the uploaded file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "    \n",
    "    # Get the filename\n",
    "    file_name = os.path.basename(zip_path)\n",
    "    \n",
    "    # Upload to Supabase storage\n",
    "    with open(zip_path, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "        \n",
    "    # Upload to model/ folder in the bucket\n",
    "    path = f\"model/{file_name}\"\n",
    "    response = supabase_client.storage.from_(bucket_name).upload(\n",
    "        path,\n",
    "        file_content,\n",
    "        file_options={\"content-type\": \"application/zip\"}\n",
    "    )\n",
    "    \n",
    "    # Generate the public URL\n",
    "    url = supabase_client.storage.from_(bucket_name).get_public_url(path)\n",
    "    \n",
    "    print(f\"Model uploaded to Supabase: {url}\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79beed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_url(supabase_client, bucket_name=\"codenection-sss-model-training\", days_ago=0):\n",
    "    \"\"\"\n",
    "    Get the URL of the latest model (or from X days ago)\n",
    "    \n",
    "    Args:\n",
    "        supabase_client: Initialized Supabase client\n",
    "        bucket_name: Name of the storage bucket\n",
    "        days_ago: How many days back to look for the model (0 = latest)\n",
    "    \n",
    "    Returns:\n",
    "        URL of the model\n",
    "    \"\"\"\n",
    "    # List all files in the model/ directory\n",
    "    files = supabase_client.storage.from_(bucket_name).list(\"model\")\n",
    "    \n",
    "    # Filter only zip files\n",
    "    model_files = [f for f in files if f[\"name\"].endswith(\".zip\")]\n",
    "    \n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No model files found in storage\")\n",
    "    \n",
    "    # Sort by name (which contains the date)\n",
    "    model_files.sort(key=lambda x: x[\"name\"], reverse=True)\n",
    "    \n",
    "    if days_ago == 0:\n",
    "        # Get the most recent model\n",
    "        target_file = model_files[0]\n",
    "    else:\n",
    "        # Try to find a model from days_ago days\n",
    "        target_date = (datetime.now() - timedelta(days=days_ago)).strftime(\"%Y%m%d\")\n",
    "        matching_files = [f for f in model_files if target_date in f[\"name\"]]\n",
    "        \n",
    "        if matching_files:\n",
    "            target_file = matching_files[0]\n",
    "        elif model_files:\n",
    "            # Fall back to the most recent model before the target date\n",
    "            earlier_files = [f for f in model_files if f[\"name\"].split(\"_\")[1].split(\".\")[0] < target_date]\n",
    "            if earlier_files:\n",
    "                target_file = earlier_files[0]\n",
    "            else:\n",
    "                target_file = model_files[-1]  # Oldest file\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model files found for {target_date} or earlier\")\n",
    "    \n",
    "    # Generate the public URL\n",
    "    path = f\"model/{target_file['name']}\"\n",
    "    url = supabase_client.storage.from_(bucket_name).get_public_url(path)\n",
    "    \n",
    "    print(f\"Found model: {target_file['name']}\")\n",
    "    return url\n",
    "\n",
    "def download_model_from_supabase(url, output_dir=None):\n",
    "    \"\"\"\n",
    "    Download a model from a Supabase URL and extract it\n",
    "    \n",
    "    Args:\n",
    "        url: URL of the model zip file\n",
    "        output_dir: Directory to extract the model to (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the extracted model directory\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import tempfile\n",
    "    \n",
    "    # Create temporary file for downloading\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_file:\n",
    "        temp_path = tmp_file.name\n",
    "    \n",
    "    # Download the file\n",
    "    print(f\"Downloading model from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(temp_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    # Create output directory if not specified\n",
    "    if output_dir is None:\n",
    "        output_dir = \"./downloaded_model\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(temp_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "    \n",
    "    # Clean up the temporary file\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "    print(f\"Model downloaded and extracted to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def load_model_from_supabase(supabase_client, days_ago=0, output_dir=None):\n",
    "    \"\"\"\n",
    "    Load a model from Supabase storage (either latest or from X days ago)\n",
    "    \n",
    "    Args:\n",
    "        supabase_client: Initialized Supabase client\n",
    "        days_ago: How many days back to look for the model (0 = latest)\n",
    "        output_dir: Directory to extract the model to (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded SentenceTransformer model\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Get the URL of the model\n",
    "    url = get_latest_model_url(supabase_client, days_ago=days_ago)\n",
    "    \n",
    "    # Download and extract the model\n",
    "    model_dir = download_model_from_supabase(url, output_dir)\n",
    "    \n",
    "    # Load the model\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    \n",
    "    print(f\"Model loaded successfully from {model_dir}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf10ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def prepare_training_examples(df):\n",
    "    \"\"\"Create training examples for fine-tuning\"\"\"\n",
    "    train_examples = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        query = row['query']\n",
    "        correction = row['correction']\n",
    "        domain = row['domain']\n",
    "        \n",
    "        # Format with context prefixes matching your ingest format\n",
    "        if domain == \"brand\":\n",
    "            # The typo should map to the correct brand embedding\n",
    "            query_text = f\"car brand: {query}\"\n",
    "            correction_text = f\"car brand: {correction}\"\n",
    "            \n",
    "            # Create training pair (these should map to the same vector)\n",
    "            train_examples.append(InputExample(texts=[query_text, correction_text]))\n",
    "            \n",
    "            # Also add reverse to strengthen the connection\n",
    "            train_examples.append(InputExample(texts=[correction_text, query_text]))\n",
    "            \n",
    "        else:  # model\n",
    "            query_text = f\"car model: {query}\"\n",
    "            correction_text = f\"car model: {correction}\"\n",
    "            \n",
    "            # Create training pair\n",
    "            train_examples.append(InputExample(texts=[query_text, correction_text]))\n",
    "            train_examples.append(InputExample(texts=[correction_text, query_text]))\n",
    "    \n",
    "    return train_examples\n",
    "\n",
    "# Path to your saved model\n",
    "EXISTING_MODEL_PATH = \"./model/finetuned-embedding-model-v3\"\n",
    "OUTPUT_PATH = \"./model/finetuned-embedding-model-v4\"  # New save location\n",
    "\n",
    "# Load the existing model\n",
    "model = SentenceTransformer(EXISTING_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3c645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b198e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supabase in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: realtime in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: supabase-functions in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: storage3 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: supabase-auth in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: postgrest in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: httpx<0.29,>=0.26 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (0.28.1)\n",
      "Requirement already satisfied: anyio in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.10.0)\n",
      "Requirement already satisfied: certifi in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
      "Requirement already satisfied: idna in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
      "Requirement already satisfied: deprecation>=2.1.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from postgrest->supabase) (2.1.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.9 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from postgrest->supabase) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from realtime->supabase) (4.15.0)\n",
      "Requirement already satisfied: websockets<16,>=11 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from realtime->supabase) (15.0.1)\n",
      "Requirement already satisfied: pyjwt[crypto]>=2.10.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase-auth->supabase) (2.10.1)\n",
      "Requirement already satisfied: strenum>=0.4.15 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase-functions->supabase) (0.4.15)\n",
      "Requirement already satisfied: packaging in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from deprecation>=2.1.0->postgrest->supabase) (25.0)\n",
      "Requirement already satisfied: h2<5,>=3 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.4.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (46.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Requirement already satisfied: cffi>=2.0.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from h2<5,>=3->httpx<0.29,>=0.26->supabase) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from h2<5,>=3->httpx<0.29,>=0.26->supabase) (4.1.0)\n",
      "Requirement already satisfied: pycparser in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "2025-09-26 13:21:49,189 - httpx - INFO - HTTP Request: GET https://cciyfbgiyqdutxdxwyxj.supabase.co/rest/v1/typo_training_dataset?select=typo%2Ccorrected%2Cdomain \"HTTP/2 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched 29 training examples from Supabase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 13:21:49,594 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-09-26 13:21:49,595 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: intfloat/multilingual-e5-small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load last week's model...\n",
      "Could not load last week's model: name 'load_model_from_supabase' is not defined. Using base model instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mretrain_model_and_upload\u001b[39m\u001b[34m(supabase_train_df)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAttempting to load last week\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m model = \u001b[43mload_model_from_supabase\u001b[49m(supabase_client, days_ago=\u001b[32m7\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully loaded model from last week: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_model_from_supabase' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Call the function to retrain and upload\u001b[39;00m\n\u001b[32m    119\u001b[39m supabase_train_df = fetch_training_data_from_supabase()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m model, model_url = \u001b[43mretrain_model_and_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupabase_train_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mretrain_model_and_upload\u001b[39m\u001b[34m(supabase_train_df)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load last week\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Using base model instead.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m supabase_train_df.empty:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Generate training examples\u001b[39;00m\n\u001b[32m     75\u001b[39m     train_examples = prepare_training_examples(supabase_train_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[32m    307\u001b[39m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[32m    308\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m has_modules = \u001b[43mis_sentence_transformer_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m    327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28mself\u001b[39m._load_sbert_model(\n\u001b[32m    328\u001b[39m         model_name_or_path,\n\u001b[32m    329\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    337\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\sentence_transformers\\util\\file_io.py:53\u001b[39m, in \u001b[36mis_sentence_transformer_model\u001b[39m\u001b[34m(model_name_or_path, token, cache_folder, revision, local_files_only)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_sentence_transformer_model\u001b[39m(\n\u001b[32m     33\u001b[39m     model_name_or_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     34\u001b[39m     token: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     local_files_only: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     38\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     39\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    Checks if the given model name or path corresponds to a SentenceTransformer model.\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m        bool: True if the model is a SentenceTransformer model, False otherwise.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         \u001b[43mload_file_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodules.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\sentence_transformers\\util\\file_io.py:96\u001b[39m, in \u001b[36mload_file_path\u001b[39m\u001b[34m(model_name_or_path, filename, subfolder, token, cache_folder, revision, local_files_only)\u001b[39m\n\u001b[32m     94\u001b[39m file_path = Path(subfolder, filename)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1073\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1071\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1097\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1099\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1544\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1545\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1550\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1551\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1460\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1474\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hf_raise_for_status(response)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:310\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\urllib3\\util\\connection.py:81\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     79\u001b[39m         err = _\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m             \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:500\u001b[39m, in \u001b[36msocket.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m, _ss=_socket.socket):\n\u001b[32m    497\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    498\u001b[39m     _ss.close(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._io_refs <= \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell to fetch training data from Supabase\n",
    "%pip install supabase\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add the parent directory to sys.path so 'app' can be imported\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from app.api.services.config import SUPABASE_ANON_KEY, SUPABASE_URL\n",
    "from supabase import create_client, Client\n",
    "\n",
    "def fetch_training_data_from_supabase():\n",
    "    \"\"\"Fetch typo correction data from Supabase\"\"\"\n",
    "    try:\n",
    "        # Initialize Supabase client\n",
    "        client = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)\n",
    "        \n",
    "        # Fetch all records from typo_training_dataset table\n",
    "        response = client.table(\"typo_training_dataset\").select(\"typo\", \"corrected\", \"domain\").execute()\n",
    "        \n",
    "        if not response.data:\n",
    "            print(\"No training data found in Supabase table\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(response.data)\n",
    "        \n",
    "        # Check required columns exist\n",
    "        required_cols = [\"typo\", \"corrected\", \"domain\"]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing required columns in Supabase data: {missing_cols}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Rename columns to match existing code\n",
    "        df = df.rename(columns={\"typo\": \"query\", \"corrected\": \"correction\"})\n",
    "        \n",
    "        print(f\"Successfully fetched {len(df)} training examples from Supabase\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from Supabase: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch data from Supabase\n",
    "def retrain_model_and_upload(supabase_train_df):\n",
    "    \"\"\"Retrain the model and upload it to Supabase\"\"\"\n",
    "    # Initialize Supabase client\n",
    "    supabase_client = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)\n",
    "    \n",
    "    # Define paths\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    LOCAL_MODEL_PATH = \"./model/finetuned-embedding-model\"\n",
    "    OUTPUT_PATH = f\"./model/finetuned-embedding-model-{today}\"\n",
    "    ZIP_PATH = f\"./model/finetuned-embedding-model-{today}.zip\"\n",
    "    \n",
    "    try:\n",
    "        # Try to load the previous week's model for continued training\n",
    "        try:\n",
    "            print(\"Attempting to load last week's model...\")\n",
    "            model = load_model_from_supabase(supabase_client, days_ago=7)\n",
    "            print(f\"Successfully loaded model from last week: {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load last week's model: {e}. Using base model instead.\")\n",
    "            model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "        if not supabase_train_df.empty:\n",
    "            # Generate training examples\n",
    "            train_examples = prepare_training_examples(supabase_train_df)\n",
    "            print(f\"Created {len(train_examples)} training examples\")\n",
    "            \n",
    "            # Create data loader\n",
    "            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n",
    "            \n",
    "            # Define loss function\n",
    "            train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Training on: {device}\")\n",
    "            \n",
    "            # Fine-tune\n",
    "            print(f\"Starting fine-tuning for 5 epochs...\")\n",
    "            warmup_steps = int(len(train_dataloader) * 5 * 0.1)\n",
    "            \n",
    "            model.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                epochs=5,\n",
    "                optimizer_params={'lr': 1e-5},\n",
    "                warmup_steps=warmup_steps,\n",
    "                output_path=OUTPUT_PATH,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Model fine-tuned and saved to {OUTPUT_PATH}\")\n",
    "            \n",
    "            # Zip the model\n",
    "            zip_path = zip_model(OUTPUT_PATH, ZIP_PATH)\n",
    "            \n",
    "            # Upload to Supabase\n",
    "            model_url = upload_model_to_supabase(zip_path, supabase_client)\n",
    "            print(f\"Model uploaded successfully: {model_url}\")\n",
    "            \n",
    "            return model, model_url\n",
    "        else:\n",
    "            print(\"No training data available. Model not trained.\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training and upload: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Call the function to retrain and upload\n",
    "supabase_train_df = fetch_training_data_from_supabase()\n",
    "model, model_url = retrain_model_and_upload(supabase_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2815ad8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create data loader for the new examples\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_dataloader = DataLoader(\u001b[43mtrain_examples\u001b[49m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size=\u001b[32m4\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define loss function\u001b[39;00m\n\u001b[32m      5\u001b[39m train_loss = losses.MultipleNegativesRankingLoss(model)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_examples' is not defined"
     ]
    }
   ],
   "source": [
    "# Create data loader for the new examples\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n",
    "\n",
    "# Define loss function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Continue fine-tuning\n",
    "print(f\"Starting additional fine-tuning for 5 epochs...\")\n",
    "warmup_steps = int(len(train_dataloader) * 5 * 0.1)  # 10% of training as warmup\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=5,  # Fewer epochs for continued training\n",
    "    optimizer_params={'lr': 1e-5},\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Model fine-tuned and saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a688c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided path: 'S:\\path\\to\\local\\model' is not a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_16608\\3256148879.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m requests\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m huggingface_hub \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m api = HfApi(token=os.getenv(\u001b[33m\"HF_TOKEN\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m api.upload_folder(\n\u001b[32m     11\u001b[39m     folder_path=\u001b[33m\"/path/to/local/model\"\u001b[39m,\n\u001b[32m     12\u001b[39m     repo_id=\u001b[33m\"ShawnSean/AutoValidate-Embedding-Model\"\u001b[39m,\n\u001b[32m     13\u001b[39m     repo_type=\u001b[33m\"model\"\u001b[39m,\n",
      "\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m             kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n\u001b[32m    113\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1665\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_as_future:\n\u001b[32m   1666\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.run_as_future(fn, self, *args, **kwargs)\n\u001b[32m   1667\u001b[39m \n\u001b[32m   1668\u001b[39m         \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1669\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(self, *args, **kwargs)\n",
      "\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[39m\n\u001b[32m   4970\u001b[39m             token=token,\n\u001b[32m   4971\u001b[39m             path_in_repo=path_in_repo,\n\u001b[32m   4972\u001b[39m             delete_patterns=delete_patterns,\n\u001b[32m   4973\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4974\u001b[39m         add_operations = self._prepare_upload_folder_additions(\n\u001b[32m   4975\u001b[39m             folder_path,\n\u001b[32m   4976\u001b[39m             path_in_repo,\n\u001b[32m   4977\u001b[39m             allow_patterns=allow_patterns,\n",
      "\u001b[32ms:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, folder_path, path_in_repo, allow_patterns, ignore_patterns, repo_type, token)\u001b[39m\n\u001b[32m   9634\u001b[39m         \"\"\"\n\u001b[32m   9635\u001b[39m \n\u001b[32m   9636\u001b[39m         folder_path = Path(folder_path).expanduser().resolve()\n\u001b[32m   9637\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m folder_path.is_dir():\n\u001b[32m-> \u001b[39m\u001b[32m9638\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"Provided path: '{folder_path}' is not a directory\")\n\u001b[32m   9639\u001b[39m \n\u001b[32m   9640\u001b[39m         \u001b[38;5;66;03m# List files from folder\u001b[39;00m\n\u001b[32m   9641\u001b[39m         relpath_to_abspath = {\n",
      "\u001b[31mValueError\u001b[39m: Provided path: 'S:\\path\\to\\local\\model' is not a directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from supabase import create_client\n",
    "import zipfile\n",
    "import requests\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"/path/to/local/model\",\n",
    "    repo_id=\"ShawnSean/AutoValidate-Embedding-Model\",\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "\n",
    "# Add the parent directory to sys.path so 'app' can be imported\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Import required functions from your notebook\n",
    "from app.api.services.config import SUPABASE_ANON_KEY, SUPABASE_URL\n",
    "\n",
    "\n",
    "def upload_model_to_supabase_direct_api(zip_path, bucket_name=\"codenection-sss-model-training\"):\n",
    "    \"\"\"\n",
    "    Upload a zipped model to Supabase storage using direct API calls and chunked upload\n",
    "    \n",
    "    Args:\n",
    "        zip_path: Path to the zipped model file\n",
    "        bucket_name: Name of the storage bucket\n",
    "    \n",
    "    Returns:\n",
    "        URL of the uploaded file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "    \n",
    "    # Get the file size\n",
    "    file_size = os.path.getsize(zip_path)\n",
    "    print(f\"File size: {file_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # Get the filename\n",
    "    file_name = os.path.basename(zip_path)\n",
    "    storage_path = f\"model/{file_name}\"\n",
    "    SUPABASE_SERVICE_KEY = os.getenv(\"SUPABASE_SERVICE_KEY\") \n",
    "    print(f\"Supabase Service Key\")\n",
    "    if not SUPABASE_SERVICE_KEY:\n",
    "        print(\"SUPABASE_SERVICE_KEY environment variable not found.\")\n",
    "        print(\"Please set it to your service role key from Supabase dashboard.\")\n",
    "        return None\n",
    "    \n",
    "    supabase_client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)\n",
    "    # Supabase API URL and headers\n",
    "    api_url = f\"{SUPABASE_URL}/storage/v1/object/{bucket_name}/{storage_path}\"\n",
    "    headers = {\n",
    "        \"apikey\": SUPABASE_ANON_KEY,\n",
    "        \"Authorization\": f\"Bearer {SUPABASE_SERVICE_KEY}\",\n",
    "        \"Content-Type\": \"application/octet-stream\",\n",
    "        \"x-upsert\": \"true\"  # Overwrite if exists\n",
    "    }\n",
    "    \n",
    "    # Upload in chunks to avoid memory issues\n",
    "    CHUNK_SIZE = 5 * 1024 * 1024  # 5MB chunks\n",
    "    \n",
    "    # Upload with requests using chunked transfer\n",
    "    with open(zip_path, 'rb') as f:\n",
    "        # Create a generator that yields chunks of the file\n",
    "        def read_in_chunks():\n",
    "            while True:\n",
    "                chunk = f.read(CHUNK_SIZE)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "        \n",
    "        print(f\"Starting upload of {file_name} to {bucket_name}/{storage_path}\")\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            data=read_in_chunks()  # Use a generator to read and send chunks\n",
    "        )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Upload successful with status code: {response.status_code}\")\n",
    "        public_url = f\"{SUPABASE_URL}/storage/v1/object/public/{bucket_name}/{storage_path}\"\n",
    "        print(f\"Public URL: {public_url}\")\n",
    "        return public_url\n",
    "    else:\n",
    "        print(f\"Upload failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        raise Exception(f\"Failed to upload file: {response.text}\")\n",
    "\n",
    "def upload_initial_model():\n",
    "    \"\"\"Upload the existing fine-tuned model to Supabase storage\"\"\"\n",
    "    \n",
    "    # Path to your existing model\n",
    "    model_path = \"./model/finetuned-embedding-model\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model directory not found at {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create a zip filename with current date\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    zip_path = f\"./model/finetuned-embedding-model-{today}.zip\"\n",
    "    \n",
    "    print(f\"1. Zipping model from {model_path}...\")\n",
    "    zip_path = zip_model_in_chunks(model_path, zip_path)\n",
    "    \n",
    "    print(f\"2. Uploading model to Supabase...\")\n",
    "    model_url = upload_model_to_supabase_direct_api(zip_path)\n",
    "    \n",
    "    print(f\"3. Model uploaded successfully!\")\n",
    "    print(f\"   URL: {model_url}\")\n",
    "    \n",
    "    return model_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting initial model upload...\")\n",
    "    try:\n",
    "        upload_initial_model()\n",
    "        print(\"Upload process completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19988746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial model upload...\n",
      "1. Zipping model from ./model/finetuned-embedding-model...\n",
      "Model zipped successfully to ./model/finetuned-embedding-model-20250926.zip\n",
      "2. Uploading model to Hugging Face Hub...\n",
      "Uploading ./model/finetuned-embedding-model-20250926.zip -> ShawnSean/AutoValidate-Embedding-Model:models/finetuned-embedding-model-20250926.zip ...\n",
      "Upload complete. URL: https://huggingface.co/ShawnSean/AutoValidate-Embedding-Model/resolve/main/models/finetuned-embedding-model-20250926.zip\n",
      "3. Model uploaded successfully!\n",
      "   URL: https://huggingface.co/ShawnSean/AutoValidate-Embedding-Model/resolve/main/models/finetuned-embedding-model-20250926.zip\n",
      "Upload process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "def zip_model_in_chunks(model_path, output_zip=None):\n",
    "    \"\"\"\n",
    "    Zip a model directory into a compressed file\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model directory\n",
    "        output_zip: Path for the output zip file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created zip file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
    "    \n",
    "    # Create a zip filename with current date if not provided\n",
    "    if output_zip is None:\n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        output_zip = f\"model_{today}.zip\"\n",
    "    \n",
    "    # Create the zip file with smaller compression for better memory usage\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_STORED) as zipf:\n",
    "        # Walk through all files in the directory\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                # Calculate path for file in zip\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(model_path))\n",
    "                \n",
    "                # Add file to zip\n",
    "                zipf.write(file_path, arcname=arcname)\n",
    "    \n",
    "    print(f\"Model zipped successfully to {output_zip}\")\n",
    "    return output_zip\n",
    "\n",
    "\n",
    "def upload_model_to_hf(zip_path, repo_id=None, repo_type=\"model\"):\n",
    "    \"\"\"\n",
    "    Upload a single file (zip) to Hugging Face Hub.\n",
    "    Requires HF_TOKEN in env and HF_REPO (or pass repo_id).\n",
    "    Returns public URL to the uploaded file in the repo.\n",
    "    \"\"\"\n",
    "    from huggingface_hub import HfApi\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    if not HF_TOKEN:\n",
    "        print(\"HF_TOKEN not set in environment.\")\n",
    "        return None\n",
    "\n",
    "    # repo id from env if not provided\n",
    "    repo_id = repo_id or os.getenv(\"HF_REPO\")\n",
    "    if not repo_id:\n",
    "        print(\"HF_REPO not set. Set env HF_REPO like 'username/repo-name' or pass repo_id.\")\n",
    "        return None\n",
    "\n",
    "    file_name = os.path.basename(zip_path)\n",
    "    path_in_repo = f\"models/{file_name}\"  # where file will live in the repo\n",
    "\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "    print(f\"Uploading {zip_path} -> {repo_id}:{path_in_repo} ...\")\n",
    "\n",
    "    # upload_file will stream the file and does not require loading whole file into memory\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=zip_path,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=repo_type,\n",
    "        token=HF_TOKEN,\n",
    "        commit_message=f\"Upload model {file_name} ({datetime.now().isoformat()})\"\n",
    "    )\n",
    "\n",
    "    url = f\"https://huggingface.co/{repo_id}/resolve/main/{path_in_repo}\"\n",
    "    print(f\"Upload complete. URL: {url}\")\n",
    "    return url\n",
    "\n",
    "def upload_initial_model():\n",
    "    \"\"\"\n",
    "    Zip local model and upload to Hugging Face Hub\n",
    "    \"\"\"\n",
    "    # Path to your existing model\n",
    "    model_path = \"./model/finetuned-embedding-model\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model directory not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "    # Create a zip filename with current date\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    zip_path = f\"./model/finetuned-embedding-model-{today}.zip\"\n",
    "\n",
    "    print(f\"1. Zipping model from {model_path}...\")\n",
    "    zip_path = zip_model_in_chunks(model_path, zip_path)  # re-uses zip (no chunking while uploading)\n",
    "\n",
    "    print(f\"2. Uploading model to Hugging Face Hub...\")\n",
    "    model_url = upload_model_to_hf(zip_path)\n",
    "    if model_url:\n",
    "        print(f\"3. Model uploaded successfully!\\n   URL: {model_url}\")\n",
    "    else:\n",
    "        print(\"Upload failed.\")\n",
    "    return model_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting initial model upload...\")\n",
    "    try:\n",
    "        upload_initial_model()\n",
    "        print(\"Upload process completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7e66d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest model: models/finetuned-embedding-model-20250926.zip (date: 20250926)\n",
      "Downloading model from Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-09-26 13:29:01,648 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "s:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aykay\\.cache\\huggingface\\hub\\models--ShawnSean--AutoValidate-Embedding-Model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 1 files: 100%|██████████| 1/1 [02:04<00:00, 124.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model to ./latest_model\\20250926...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 13:29:58,997 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-09-26 13:29:58,999 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: ./latest_model\\20250926\\finetuned-embedding-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./latest_model\\20250926\\finetuned-embedding-model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from huggingface_hub import snapshot_download, HfApi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_model_from_hf(\n",
    "    repo_id=\"ShawnSean/AutoValidate-Embedding-Model\", \n",
    "    model_date=None, \n",
    "    use_temp=True,\n",
    "    cache_dir=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Download and load a model from Hugging Face Hub\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Hugging Face repo ID (default: \"ShawnSean/AutoValidate-Embedding-Model\")\n",
    "        model_date: Specific date of model to load (format: YYYYMMDD, default: latest)\n",
    "        use_temp: Whether to use a temporary directory (deleted on program exit)\n",
    "        cache_dir: Custom cache directory (if use_temp=False)\n",
    "        \n",
    "    Returns:\n",
    "        Loaded SentenceTransformer model\n",
    "    \"\"\"\n",
    "    # Set up HF_TOKEN from env if not already in environment\n",
    "    if \"HF_TOKEN\" not in os.environ:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "    \n",
    "    # Disable progress bars for cleaner output\n",
    "    os.environ[\"HF_HUB_DISABLE_PROGRESS\"] = \"1\"\n",
    "    \n",
    "    # Get all model files in the repo to find the latest if no date specified\n",
    "    api = HfApi()\n",
    "    \n",
    "    try:\n",
    "        # List model files in the repo\n",
    "        model_files = [\n",
    "            f for f in api.list_repo_files(repo_id=repo_id)\n",
    "            if f.startswith(\"models/finetuned-embedding-model-\") and f.endswith(\".zip\")\n",
    "        ]\n",
    "        \n",
    "        if not model_files:\n",
    "            raise FileNotFoundError(f\"No model files found in {repo_id}\")\n",
    "        \n",
    "        # Extract the date from filename\n",
    "        date_from_file = None\n",
    "        \n",
    "        if model_date is None:\n",
    "            # Find the latest model if no specific date requested\n",
    "            # Sort by date in filename\n",
    "            model_files.sort(reverse=True)\n",
    "            model_file = model_files[0]\n",
    "            # Extract date from filename (e.g., finetuned-embedding-model-20250925.zip -> 20250925)\n",
    "            date_from_file = model_file.split('-')[-1].split('.')[0]\n",
    "            print(f\"Using latest model: {model_file} (date: {date_from_file})\")\n",
    "        else:\n",
    "            # Find the model with the specified date\n",
    "            date_str = str(model_date)\n",
    "            matching_files = [f for f in model_files if date_str in f]\n",
    "            if not matching_files:\n",
    "                raise FileNotFoundError(f\"No model found for date {model_date}\")\n",
    "            model_file = matching_files[0]\n",
    "            date_from_file = date_str\n",
    "            print(f\"Using model from {model_date}: {model_file}\")\n",
    "        \n",
    "        # Create extract directory with date\n",
    "        base_dir = cache_dir if not use_temp else \"./latest_model\"\n",
    "        extract_dir = os.path.join(base_dir, date_from_file)\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the final model directory path\n",
    "        model_dir = os.path.join(extract_dir, \"finetuned-embedding-model\")\n",
    "        \n",
    "        # Skip download if model already exists\n",
    "        if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
    "            print(f\"Model already exists at {model_dir}, skipping download\")\n",
    "        else:\n",
    "            # Download the specific file from the repo\n",
    "            print(f\"Downloading model from Hugging Face Hub...\")\n",
    "            repo_dir = snapshot_download(\n",
    "                repo_id=repo_id,\n",
    "                allow_patterns=[model_file],\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            \n",
    "            # Path to the downloaded zip file\n",
    "            zip_path = os.path.join(repo_dir, model_file)\n",
    "            \n",
    "            # Extract the model\n",
    "            print(f\"Extracting model to {extract_dir}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "            \n",
    "            # Clean up the download cache to save space\n",
    "            if os.path.exists(repo_dir) and repo_dir != extract_dir:\n",
    "                shutil.rmtree(repo_dir)\n",
    "        \n",
    "        # Load the model with SentenceTransformer\n",
    "        print(f\"Loading model from {model_dir}...\")\n",
    "        model = SentenceTransformer(model_dir)\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from Hugging Face: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the latest model\n",
    "    model = load_model_from_hf()\n",
    "    \n",
    "    # Or load a specific date's model\n",
    "    # model = load_model_from_hf(model_date=\"20250925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ded14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
