{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6521f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset \n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.append(notebook_dir)\n",
    "\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "OUTPUT_PATH = \"./model/finetuned-embedding-model\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47da554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL')\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce753c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a7054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "collection_name = \"car_data_modelbrand\"  \n",
    "\n",
    "# Function to search car brands with typo tolerance\n",
    "def search_car_brand(query, top_k=10):\n",
    "    # Add context to the query to match our embeddings\n",
    "    query_with_context = f\"car brand: {query}\" \n",
    "    query_vector = model.encode(query_with_context)\n",
    "    \n",
    "    # Use query_filter instead of filter parameter\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k * 3,\n",
    "        query_filter={\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"key\": \"vector_type\",\n",
    "                    \"match\": {\n",
    "                        \"value\": \"brand\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Remove duplicates based on car_brand\n",
    "    unique_results_brand = {}\n",
    "    for result in search_result:\n",
    "        car_model = result.payload['car_brand']\n",
    "        if car_model not in unique_results_brand:\n",
    "            unique_results_brand[car_model] = result\n",
    "    \n",
    "    # Return top_k unique results\n",
    "    return list(unique_results_brand.values())[:top_k]\n",
    "\n",
    "\n",
    "# Function to search car models with typo tolerance\n",
    "def search_car_model(query, top_k=10):\n",
    "    # Add generic context to the query to match our embeddings\n",
    "    query_with_context = f\"car model: {query}\"  \n",
    "    query_vector = model.encode(query_with_context)\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k * 3,\n",
    "        query_filter={\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"key\": \"vector_type\",\n",
    "                    \"match\": {\n",
    "                        \"value\": \"model\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Remove duplicates based on car_model\n",
    "    unique_results_model = {}\n",
    "    for result in search_result:\n",
    "        car_model = result.payload['car_model']\n",
    "        if car_model not in unique_results_model:\n",
    "            unique_results_model[car_model] = result\n",
    "    \n",
    "    # Return top_k unique results\n",
    "    return list(unique_results_model.values())[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41a77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process as rapidfuzz_process\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text to improve matching\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r'[\\-–—_/]', ' ', s)  \n",
    "    s = re.sub(r'\\s+', ' ', s)     \n",
    "    return s  \n",
    "\n",
    "def hybrid_search(query, choices, vector_type=\"brand\", fuzzy_threshold=75, top_k=3, search_model = None):\n",
    "    \"\"\"\n",
    "    Hybrid search that combines RapidFuzz and embeddings:\n",
    "    1. Try fuzzy matching first with 75% threshold (fast + handles typos)\n",
    "    2. If no good fuzzy matches, fall back to embeddings (semantic understanding)\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        choices: List of choices to search against\n",
    "        vector_type: \"brand\" or \"model\"\n",
    "        fuzzy_threshold: Minimum score (0-100) for fuzzy matches\n",
    "        top_k: Number of results to return\n",
    "    \"\"\"\n",
    "    # Normalize query for better matching\n",
    "    query_norm = normalize_text(query)\n",
    "\n",
    "    model_to_use = search_model if search_model is not None else model\n",
    "\n",
    "    \n",
    "    # Adjust scorer based on query characteristics\n",
    "    if ' ' in query_norm or len(query_norm) > 10:\n",
    "        scorer = fuzz.token_sort_ratio  # Better for word order/spacing differences\n",
    "    else:\n",
    "        scorer = fuzz.ratio  # Standard for character-level typos\n",
    "    \n",
    "    # Step 1: Try RapidFuzz first (faster than embeddings)\n",
    "    fuzzy_matches = rapidfuzz_process.extract(\n",
    "        query_norm, \n",
    "        [normalize_text(c) for c in choices],  # Normalize choices too\n",
    "        scorer=scorer,\n",
    "        limit=top_k * 2  # Get more candidates for filtering\n",
    "    )\n",
    "    \n",
    "    # Map normalized choices back to original labels\n",
    "    norm_to_orig = {normalize_text(c): c for c in choices}\n",
    "    fuzzy_matches = [(norm_to_orig.get(match, match), score, idx) for match, score, idx in fuzzy_matches]\n",
    "    \n",
    "    # Filter matches that meet our threshold\n",
    "    good_fuzzy_matches = [(match, score) for match, score, _ in fuzzy_matches if score >= fuzzy_threshold]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # If we have good fuzzy matches, return those\n",
    "    if good_fuzzy_matches:\n",
    "        print(f\"Found {len(good_fuzzy_matches)} good fuzzy matches for '{query}'\")\n",
    "        for match, score in good_fuzzy_matches:\n",
    "            results.append({\n",
    "                \"text\": match,\n",
    "                \"score\": score / 100.0,  # Normalize to 0-1 scale\n",
    "                \"source\": \"fuzzy\"\n",
    "            })\n",
    "    \n",
    "    # Step 2: If not enough good fuzzy matches, use embeddings\n",
    "    if len(results) == 0:\n",
    "        print(f\"No good fuzzy matches above threshold {fuzzy_threshold}, using embeddings\")\n",
    "        \n",
    "        # Use appropriate search function based on vector_type\n",
    "        if vector_type == \"brand\":\n",
    "            # Modify how you call search_car_brand to use the provided model\n",
    "            query_with_context = f\"car brand: {query}\"\n",
    "            query_vector = model_to_use.encode(query_with_context)\n",
    "            \n",
    "            # Call Qdrant directly with the new embedding\n",
    "            search_result = qdrant_client.search(\n",
    "                collection_name=collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k * 3,\n",
    "                query_filter={\"must\": [{\"key\": \"vector_type\", \"match\": {\"value\": \"brand\"}}]}\n",
    "            )\n",
    "            \n",
    "            # Process results as in search_car_brand\n",
    "            embedding_results = []\n",
    "            unique_results = {}\n",
    "            for result in search_result:\n",
    "                car_brand = result.payload['car_brand']\n",
    "                if car_brand not in unique_results:\n",
    "                    unique_results[car_brand] = result\n",
    "                    embedding_results.append(result)\n",
    "        else:  # model\n",
    "            # Similar for model search\n",
    "            query_with_context = f\"car model: {query}\"\n",
    "            query_vector = model_to_use.encode(query_with_context)\n",
    "            \n",
    "            # Call Qdrant directly\n",
    "            search_result = qdrant_client.search(\n",
    "                collection_name=collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k * 3,\n",
    "                query_filter={\"must\": [{\"key\": \"vector_type\", \"match\": {\"value\": \"model\"}}]}\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            embedding_results = []\n",
    "            unique_results = {}\n",
    "            for result in search_result:\n",
    "                car_model = result.payload['car_model']\n",
    "                if car_model not in unique_results:\n",
    "                    unique_results[car_model] = result\n",
    "                    embedding_results.append(result)\n",
    "        \n",
    "        # Extract relevant information\n",
    "        for result in embedding_results:\n",
    "            if vector_type == \"brand\":\n",
    "                text = result.payload.get(\"car_brand\")\n",
    "            else:  # model\n",
    "                text = result.payload.get(\"car_model\")\n",
    "                \n",
    "            # Skip if this result is already in our list from fuzzy matching\n",
    "            if any(r[\"text\"] == text for r in results):\n",
    "                continue\n",
    "                \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"score\": result.score,\n",
    "                \"source\": \"embedding\"\n",
    "            })\n",
    "    \n",
    "    # Return top_k results, sorted by score\n",
    "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079951bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correction</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neesun</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>benz</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>merz</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mercedesbenz</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toyata</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>toyta</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hunda</td>\n",
       "      <td>Honda</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hoonda</td>\n",
       "      <td>Honda</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>protan</td>\n",
       "      <td>Proton</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>perodwa</td>\n",
       "      <td>Perodua</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>axla</td>\n",
       "      <td>Axia</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xseventy</td>\n",
       "      <td>X70</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vios</td>\n",
       "      <td>Vios</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vios</td>\n",
       "      <td>Vios</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sivic</td>\n",
       "      <td>Civic</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>civek</td>\n",
       "      <td>Civic</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cityy</td>\n",
       "      <td>City</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>x fivty</td>\n",
       "      <td>X50</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>exora</td>\n",
       "      <td>Exora</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           query     correction domain\n",
       "0         neesun         Nissan  brand\n",
       "1           benz  Mercedes-Benz  brand\n",
       "2           merz  Mercedes-Benz  brand\n",
       "3   mercedesbenz  Mercedes-Benz  brand\n",
       "4         toyata         Toyota  brand\n",
       "5          toyta         Toyota  brand\n",
       "6          hunda          Honda  brand\n",
       "7         hoonda          Honda  brand\n",
       "8         protan         Proton  brand\n",
       "9        perodwa        Perodua  brand\n",
       "10          axla           Axia  model\n",
       "11      xseventy            X70  model\n",
       "12          vios           Vios  model\n",
       "13          vios           Vios  model\n",
       "14         sivic          Civic  model\n",
       "15         civek          Civic  model\n",
       "16         cityy           City  model\n",
       "17       x fivty            X50  model\n",
       "18         exora          Exora  model"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Collect training pairs from your examples\n",
    "\n",
    "# Brand test cases\n",
    "brand_typo_pairs = [\n",
    "    (\"neesun\", \"Nissan\"),\n",
    "    (\"benz\", \"Mercedes-Benz\"),\n",
    "    (\"merz\", \"Mercedes-Benz\"),\n",
    "    (\"mercedesbenz\", \"Mercedes-Benz\"),\n",
    "    (\"toyata\", \"Toyota\"),\n",
    "    (\"toyta\", \"Toyota\"),\n",
    "    (\"hunda\", \"Honda\"),\n",
    "    (\"hoonda\", \"Honda\"),\n",
    "    (\"protan\", \"Proton\"),\n",
    "    (\"perodwa\", \"Perodua\"),\n",
    "]\n",
    "\n",
    "# Model test cases - expanded\n",
    "model_typo_pairs = [\n",
    "    (\"axla\", \"Axia\"),\n",
    "    (\"xseventy\", \"X70\"),\n",
    "    (\"vios\", \"Vios\"),\n",
    "    (\"vios\", \"Vios\"),\n",
    "    (\"sivic\", \"Civic\"),\n",
    "    (\"civek\", \"Civic\"),\n",
    "    (\"cityy\", \"City\"),\n",
    "    (\"x fivty\", \"X50\"),\n",
    "    (\"exora\", \"Exora\"),\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame([\n",
    "    {\"query\": query, \"correction\": correction, \"domain\": \"brand\"} \n",
    "    for query, correction in brand_typo_pairs\n",
    "] + [\n",
    "    {\"query\": query, \"correction\": correction, \"domain\": \"model\"} \n",
    "    for query, correction in model_typo_pairs\n",
    "])\n",
    "\n",
    "print(f\"Total training pairs: {len(train_df)}\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5284d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_examples\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create training examples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m train_examples = prepare_training_examples(\u001b[43mtrain_df\u001b[49m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_examples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training examples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Create sentence-transformers training examples\n",
    "\n",
    "\n",
    "\n",
    "# Create training examples\n",
    "train_examples = prepare_training_examples(train_df)\n",
    "print(f\"Created {len(train_examples)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a5951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING BRAND TYPO DETECTION\n",
      "\n",
      "No good fuzzy matches above threshold 75, using embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aykay\\AppData\\Local\\Temp\\ipykernel_22416\\3121691710.py:77: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'neesun' (expected: Nissan)\n",
      "  1. Perodua (0.9028, embedding)  \n",
      "  2. Nissan (0.9001, embedding) ✓\n",
      "  3. Proton (0.8953, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'benz' (expected: Mercedes-Benz)\n",
      "  1. Perodua (0.9207, embedding)  \n",
      "  2. Mercedes-Benz (0.9107, embedding) ✓\n",
      "  3. Toyota (0.9030, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'merz' (expected: Mercedes-Benz)\n",
      "  1. Chery (0.9141, embedding)  \n",
      "  2. Perodua (0.9092, embedding)  \n",
      "  3. Mercedes-Benz (0.8999, embedding) ✓\n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'mercedesbenz' (expected: Mercedes-Benz)\n",
      "  1. Perodua (0.9236, embedding)  \n",
      "  2. Mercedes-Benz (0.9232, embedding) ✓\n",
      "  3. Chery (0.9030, embedding)  \n",
      "------------------------------------------------------------\n",
      "\n",
      "=== BRAND TYPO DETECTION SUMMARY ===\n",
      "Total cases: 4\n",
      "Resolved by fuzzy: 0 (0.0%)\n",
      "Resolved by embeddings: 4 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# 5. Load car data for testing\n",
    "df = pd.read_csv('car_dataset.csv')\n",
    "brand_choices = list(df['car_brand'].unique())\n",
    "model_choices = list(df['car_model'].unique())\n",
    "\n",
    "# Create test cases based on your examples\n",
    "brand_eval_cases = [\n",
    "    (\"neesun\", \"Nissan\"),\n",
    "    (\"benz\", \"Mercedes-Benz\"),\n",
    "    (\"merz\", \"Mercedes-Benz\"),\n",
    "    (\"mercedesbenz\", \"Mercedes-Benz\"),\n",
    "]\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(\"\\nTESTING BRAND TYPO DETECTION\\n\")\n",
    "\n",
    "# Track stats\n",
    "brand_stats = {\"total\": len(brand_eval_cases), \"fuzzy\": 0, \"embedding\": 0}\n",
    "\n",
    "for query, expected in brand_eval_cases:\n",
    "    # Test brand search\n",
    "    results = hybrid_search(query, brand_choices, vector_type=\"brand\", fuzzy_threshold=75, top_k=3, search_model=model)\n",
    "    \n",
    "    # Track which method provided the results\n",
    "    if results and results[0][\"source\"] == \"fuzzy\":\n",
    "        brand_stats[\"fuzzy\"] += 1\n",
    "    elif results:\n",
    "        brand_stats[\"embedding\"] += 1\n",
    "        \n",
    "    # Print results\n",
    "    print(f\"\\nQuery: '{query}' (expected: {expected})\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        match = \"✓\" if res[\"text\"] == expected else \" \"\n",
    "        print(f\"  {i}. {res['text']} ({res['score']:.4f}, {res['source']}) {match}\")\n",
    "    \n",
    "    # Visual separator\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\n=== BRAND TYPO DETECTION SUMMARY ===\")\n",
    "print(f\"Total cases: {brand_stats['total']}\")\n",
    "print(f\"Resolved by fuzzy: {brand_stats['fuzzy']} ({brand_stats['fuzzy']/brand_stats['total']*100:.1f}%)\")\n",
    "print(f\"Resolved by embeddings: {brand_stats['embedding']} ({brand_stats['embedding']/brand_stats['total']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINE-TUNING THE MODEL\n",
      "\n",
      "Training on: cpu\n",
      "Starting fine-tuning for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to ./model/finetuned-embedding-model\n"
     ]
    }
   ],
   "source": [
    "# 6. Fine-tune the model\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "print(\"\\nFINE-TUNING THE MODEL\\n\")\n",
    "\n",
    "# Load base model for fine-tuning\n",
    "fine_tune_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Create data loader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define loss function - we use contrastive loss to make similar pairs closer\n",
    "train_loss = losses.MultipleNegativesRankingLoss(fine_tune_model)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Fine-tune\n",
    "print(f\"Starting fine-tuning for {EPOCHS} epochs...\")\n",
    "warmup_steps = int(len(train_dataloader) * EPOCHS * 0.1)  # 10% of training as warmup\n",
    "\n",
    "fine_tune_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Model fine-tuned and saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b4475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING FINE-TUNED MODEL\n",
      "\n",
      "\n",
      "FINE-TUNED MODEL RESULTS\n",
      "\n",
      "No good fuzzy matches above threshold 75, using embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aykay\\AppData\\Local\\Temp\\ipykernel_22416\\3121691710.py:77: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'neesun' (expected: Nissan)\n",
      "  1. Nissan (0.6039, embedding) ✓\n",
      "  2. Toyota (0.4988, embedding)  \n",
      "  3. Mitsubishi (0.4841, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'benz' (expected: Mercedes-Benz)\n",
      "  1. Mercedes-Benz (0.5434, embedding) ✓\n",
      "  2. Perodua (0.5218, embedding)  \n",
      "  3. BMW (0.4875, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'merz' (expected: Mercedes-Benz)\n",
      "  1. Mercedes-Benz (0.5536, embedding) ✓\n",
      "  2. Chery (0.5491, embedding)  \n",
      "  3. Perodua (0.5171, embedding)  \n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'mercedesbenz' (expected: Mercedes-Benz)\n",
      "  1. Mercedes-Benz (0.6245, embedding) ✓\n",
      "  2. Perodua (0.5866, embedding)  \n",
      "  3. Chery (0.5607, embedding)  \n",
      "------------------------------------------------------------\n",
      "Found 2 good fuzzy matches for 'axla'\n",
      "\n",
      "Query: 'axla' (expected: Axia)\n",
      "  1. Alza (0.7500, fuzzy)  \n",
      "  2. Axia (0.7500, fuzzy) ✓\n",
      "------------------------------------------------------------\n",
      "No good fuzzy matches above threshold 75, using embeddings\n",
      "\n",
      "Query: 'xseventy' (expected: X70)\n",
      "  1. X70 (0.6537, embedding) ✓\n",
      "  2. X90 (0.6190, embedding)  \n",
      "  3. CX-8 (0.6161, embedding)  \n",
      "------------------------------------------------------------\n",
      "\n",
      "=== FINE-TUNED MODEL DETECTION SUMMARY ===\n",
      "Total test cases: 6\n",
      "\n",
      "Brand cases: 4\n",
      "Resolved by fuzzy: 0 (0.0%)\n",
      "Resolved by embeddings: 4 (100.0%)\n",
      "\n",
      "Model cases: 2\n",
      "Resolved by fuzzy: 1 (50.0%)\n",
      "Resolved by embeddings: 1 (50.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aykay\\AppData\\Local\\Temp\\ipykernel_22416\\3121691710.py:98: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "# 7. Test with fine-tuned model\n",
    "\n",
    "print(\"\\nTESTING FINE-TUNED MODEL\\n\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "finetuned_model = SentenceTransformer(OUTPUT_PATH)\n",
    "\n",
    "# Create test cases based on your examples - include both brand and model\n",
    "test_cases = [\n",
    "    # Brand test cases\n",
    "    (\"neesun\", \"Nissan\", \"brand\"),\n",
    "    (\"benz\", \"Mercedes-Benz\", \"brand\"),\n",
    "    (\"merz\", \"Mercedes-Benz\", \"brand\"),\n",
    "    (\"mercedesbenz\", \"Mercedes-Benz\", \"brand\"),\n",
    "    # Model test cases\n",
    "    (\"axla\", \"Axia\", \"model\"),\n",
    "    (\"xseventy\", \"X70\", \"model\"),\n",
    "]\n",
    "\n",
    "# Track stats\n",
    "finetuned_stats = {\"total\": 0, \"brand\": {\"fuzzy\": 0, \"embedding\": 0}, \"model\": {\"fuzzy\": 0, \"embedding\": 0}}\n",
    "\n",
    "# Count total cases by type\n",
    "finetuned_stats[\"brand_total\"] = sum(1 for _, _, domain in test_cases if domain == \"brand\")\n",
    "finetuned_stats[\"model_total\"] = sum(1 for _, _, domain in test_cases if domain == \"model\")\n",
    "finetuned_stats[\"total\"] = len(test_cases)\n",
    "\n",
    "print(\"\\nFINE-TUNED MODEL RESULTS\\n\")\n",
    "\n",
    "for query, expected, domain in test_cases:\n",
    "    # Set choices based on domain\n",
    "    choices = brand_choices if domain == \"brand\" else model_choices\n",
    "    \n",
    "    # Test search with fine-tuned model\n",
    "    results = hybrid_search(\n",
    "        query, choices, vector_type=domain, \n",
    "        fuzzy_threshold=75, top_k=3, search_model=finetuned_model\n",
    "    )\n",
    "    \n",
    "    # Track which method provided the results\n",
    "    if results and results[0][\"source\"] == \"fuzzy\":\n",
    "        finetuned_stats[domain][\"fuzzy\"] += 1\n",
    "    elif results:\n",
    "        finetuned_stats[domain][\"embedding\"] += 1\n",
    "        \n",
    "    # Print results\n",
    "    print(f\"\\nQuery: '{query}' (expected: {expected})\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        match = \"✓\" if res[\"text\"] == expected else \" \"\n",
    "        print(f\"  {i}. {res['text']} ({res['score']:.4f}, {res['source']}) {match}\")\n",
    "    \n",
    "    # Visual separator\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\n=== FINE-TUNED MODEL DETECTION SUMMARY ===\")\n",
    "print(f\"Total test cases: {finetuned_stats['total']}\")\n",
    "\n",
    "print(f\"\\nBrand cases: {finetuned_stats['brand_total']}\")\n",
    "if finetuned_stats['brand_total'] > 0:\n",
    "    print(f\"Resolved by fuzzy: {finetuned_stats['brand']['fuzzy']} ({finetuned_stats['brand']['fuzzy']/finetuned_stats['brand_total']*100:.1f}%)\")\n",
    "    print(f\"Resolved by embeddings: {finetuned_stats['brand']['embedding']} ({finetuned_stats['brand']['embedding']/finetuned_stats['brand_total']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nModel cases: {finetuned_stats['model_total']}\")\n",
    "if finetuned_stats['model_total'] > 0:\n",
    "    print(f\"Resolved by fuzzy: {finetuned_stats['model']['fuzzy']} ({finetuned_stats['model']['fuzzy']/finetuned_stats['model_total']*100:.1f}%)\")\n",
    "    print(f\"Resolved by embeddings: {finetuned_stats['model']['embedding']} ({finetuned_stats['model']['embedding']/finetuned_stats['model_total']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e8e74",
   "metadata": {},
   "source": [
    "## Retrain the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6731b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from supabase import create_client\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def zip_model(model_path, output_zip=None):\n",
    "    \"\"\"\n",
    "    Zip a model directory into a compressed file\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model directory\n",
    "        output_zip: Path for the output zip file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created zip file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
    "    \n",
    "    # Create a zip filename with current date if not provided\n",
    "    if output_zip is None:\n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        output_zip = f\"model_{today}.zip\"\n",
    "    \n",
    "    # Create the zip file\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Walk through all files in the directory\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                # Calculate path for file in zip\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(model_path))\n",
    "                \n",
    "                # Add file to zip\n",
    "                zipf.write(file_path, arcname=arcname)\n",
    "    \n",
    "    print(f\"Model zipped successfully to {output_zip}\")\n",
    "    return output_zip\n",
    "\n",
    "def upload_model_to_supabase(zip_path, supabase_client, bucket_name=\"codenection-sss-model-training\"):\n",
    "    \"\"\"\n",
    "    Upload a zipped model to Supabase storage\n",
    "    \n",
    "    Args:\n",
    "        zip_path: Path to the zipped model file\n",
    "        supabase_client: Initialized Supabase client\n",
    "        bucket_name: Name of the storage bucket\n",
    "    \n",
    "    Returns:\n",
    "        URL of the uploaded file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "    \n",
    "    # Get the filename\n",
    "    file_name = os.path.basename(zip_path)\n",
    "    \n",
    "    # Upload to Supabase storage\n",
    "    with open(zip_path, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "        \n",
    "    # Upload to model/ folder in the bucket\n",
    "    path = f\"model/{file_name}\"\n",
    "    response = supabase_client.storage.from_(bucket_name).upload(\n",
    "        path,\n",
    "        file_content,\n",
    "        file_options={\"content-type\": \"application/zip\"}\n",
    "    )\n",
    "    \n",
    "    # Generate the public URL\n",
    "    url = supabase_client.storage.from_(bucket_name).get_public_url(path)\n",
    "    \n",
    "    print(f\"Model uploaded to Supabase: {url}\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79beed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_url(supabase_client, bucket_name=\"codenection-sss-model-training\", days_ago=0):\n",
    "    \"\"\"\n",
    "    Get the URL of the latest model (or from X days ago)\n",
    "    \n",
    "    Args:\n",
    "        supabase_client: Initialized Supabase client\n",
    "        bucket_name: Name of the storage bucket\n",
    "        days_ago: How many days back to look for the model (0 = latest)\n",
    "    \n",
    "    Returns:\n",
    "        URL of the model\n",
    "    \"\"\"\n",
    "    # List all files in the model/ directory\n",
    "    files = supabase_client.storage.from_(bucket_name).list(\"model\")\n",
    "    \n",
    "    # Filter only zip files\n",
    "    model_files = [f for f in files if f[\"name\"].endswith(\".zip\")]\n",
    "    \n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No model files found in storage\")\n",
    "    \n",
    "    # Sort by name (which contains the date)\n",
    "    model_files.sort(key=lambda x: x[\"name\"], reverse=True)\n",
    "    \n",
    "    if days_ago == 0:\n",
    "        # Get the most recent model\n",
    "        target_file = model_files[0]\n",
    "    else:\n",
    "        # Try to find a model from days_ago days\n",
    "        target_date = (datetime.now() - timedelta(days=days_ago)).strftime(\"%Y%m%d\")\n",
    "        matching_files = [f for f in model_files if target_date in f[\"name\"]]\n",
    "        \n",
    "        if matching_files:\n",
    "            target_file = matching_files[0]\n",
    "        elif model_files:\n",
    "            # Fall back to the most recent model before the target date\n",
    "            earlier_files = [f for f in model_files if f[\"name\"].split(\"_\")[1].split(\".\")[0] < target_date]\n",
    "            if earlier_files:\n",
    "                target_file = earlier_files[0]\n",
    "            else:\n",
    "                target_file = model_files[-1]  # Oldest file\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model files found for {target_date} or earlier\")\n",
    "    \n",
    "    # Generate the public URL\n",
    "    path = f\"model/{target_file['name']}\"\n",
    "    url = supabase_client.storage.from_(bucket_name).get_public_url(path)\n",
    "    \n",
    "    print(f\"Found model: {target_file['name']}\")\n",
    "    return url\n",
    "\n",
    "def download_model_from_supabase(url, output_dir=None):\n",
    "    \"\"\"\n",
    "    Download a model from a Supabase URL and extract it\n",
    "    \n",
    "    Args:\n",
    "        url: URL of the model zip file\n",
    "        output_dir: Directory to extract the model to (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the extracted model directory\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import tempfile\n",
    "    \n",
    "    # Create temporary file for downloading\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_file:\n",
    "        temp_path = tmp_file.name\n",
    "    \n",
    "    # Download the file\n",
    "    print(f\"Downloading model from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(temp_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    # Create output directory if not specified\n",
    "    if output_dir is None:\n",
    "        output_dir = \"./downloaded_model\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(temp_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "    \n",
    "    # Clean up the temporary file\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "    print(f\"Model downloaded and extracted to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def load_model_from_supabase(supabase_client, days_ago=0, output_dir=None):\n",
    "    \"\"\"\n",
    "    Load a model from Supabase storage (either latest or from X days ago)\n",
    "    \n",
    "    Args:\n",
    "        supabase_client: Initialized Supabase client\n",
    "        days_ago: How many days back to look for the model (0 = latest)\n",
    "        output_dir: Directory to extract the model to (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded SentenceTransformer model\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Get the URL of the model\n",
    "    url = get_latest_model_url(supabase_client, days_ago=days_ago)\n",
    "    \n",
    "    # Download and extract the model\n",
    "    model_dir = download_model_from_supabase(url, output_dir)\n",
    "    \n",
    "    # Load the model\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    \n",
    "    print(f\"Model loaded successfully from {model_dir}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf10ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 21:47:35,037 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-09-24 21:47:35,048 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: ./model/finetuned-embedding-model-v2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def prepare_training_examples(df):\n",
    "    \"\"\"Create training examples for fine-tuning\"\"\"\n",
    "    train_examples = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        query = row['query']\n",
    "        correction = row['correction']\n",
    "        domain = row['domain']\n",
    "        \n",
    "        # Format with context prefixes matching your ingest format\n",
    "        if domain == \"brand\":\n",
    "            # The typo should map to the correct brand embedding\n",
    "            query_text = f\"car brand: {query}\"\n",
    "            correction_text = f\"car brand: {correction}\"\n",
    "            \n",
    "            # Create training pair (these should map to the same vector)\n",
    "            train_examples.append(InputExample(texts=[query_text, correction_text]))\n",
    "            \n",
    "            # Also add reverse to strengthen the connection\n",
    "            train_examples.append(InputExample(texts=[correction_text, query_text]))\n",
    "            \n",
    "        else:  # model\n",
    "            query_text = f\"car model: {query}\"\n",
    "            correction_text = f\"car model: {correction}\"\n",
    "            \n",
    "            # Create training pair\n",
    "            train_examples.append(InputExample(texts=[query_text, correction_text]))\n",
    "            train_examples.append(InputExample(texts=[correction_text, query_text]))\n",
    "    \n",
    "    return train_examples\n",
    "\n",
    "# Path to your saved model\n",
    "EXISTING_MODEL_PATH = \"./model/finetuned-embedding-model-v2\"\n",
    "OUTPUT_PATH = \"./model/finetuned-embedding-model-v3\"  # New save location\n",
    "\n",
    "# Load the existing model\n",
    "model = SentenceTransformer(EXISTING_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3c645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b198e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supabase in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: realtime in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: supabase-functions in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: storage3 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: supabase-auth in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: postgrest in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (2.19.0)\n",
      "Requirement already satisfied: httpx<0.29,>=0.26 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase) (0.28.1)\n",
      "Requirement already satisfied: anyio in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.10.0)\n",
      "Requirement already satisfied: certifi in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
      "Requirement already satisfied: idna in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
      "Requirement already satisfied: deprecation>=2.1.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from postgrest->supabase) (2.1.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.9 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from postgrest->supabase) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from realtime->supabase) (4.15.0)\n",
      "Requirement already satisfied: websockets<16,>=11 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from realtime->supabase) (15.0.1)\n",
      "Requirement already satisfied: pyjwt[crypto]>=2.10.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase-auth->supabase) (2.10.1)\n",
      "Requirement already satisfied: strenum>=0.4.15 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from supabase-functions->supabase) (0.4.15)\n",
      "Requirement already satisfied: packaging in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from deprecation>=2.1.0->postgrest->supabase) (25.0)\n",
      "Requirement already satisfied: h2<5,>=3 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.4.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (46.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Requirement already satisfied: cffi>=2.0.0 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from h2<5,>=3->httpx<0.29,>=0.26->supabase) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from h2<5,>=3->httpx<0.29,>=0.26->supabase) (4.1.0)\n",
      "Requirement already satisfied: pycparser in s:\\projects\\vehicle-insurance-backend-api\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "2025-09-24 21:47:59,486 - httpx - INFO - HTTP Request: GET https://cciyfbgiyqdutxdxwyxj.supabase.co/rest/v1/typo_training_dataset?select=typo%2Ccorrected%2Cdomain \"HTTP/2 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched 17 training examples from Supabase\n",
      "Created 34 combined training examples\n",
      "Training on: cpu\n",
      "Starting additional fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 01:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 21:49:34,344 - sentence_transformers.SentenceTransformer - INFO - Save model to ./model/finetuned-embedding-model-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to ./model/finetuned-embedding-model-v3\n"
     ]
    }
   ],
   "source": [
    "# Cell to fetch training data from Supabase\n",
    "%pip install supabase\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path so 'app' can be imported\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from app.api.services.config import SUPABASE_ANON_KEY, SUPABASE_URL\n",
    "from supabase import create_client, Client\n",
    "\n",
    "def fetch_training_data_from_supabase():\n",
    "    \"\"\"Fetch typo correction data from Supabase\"\"\"\n",
    "    try:\n",
    "        # Initialize Supabase client\n",
    "        client = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)\n",
    "        \n",
    "        # Fetch all records from typo_training_dataset table\n",
    "        response = client.table(\"typo_training_dataset\").select(\"typo\", \"corrected\", \"domain\").execute()\n",
    "        \n",
    "        if not response.data:\n",
    "            print(\"No training data found in Supabase table\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(response.data)\n",
    "        \n",
    "        # Check required columns exist\n",
    "        required_cols = [\"typo\", \"corrected\", \"domain\"]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing required columns in Supabase data: {missing_cols}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Rename columns to match existing code\n",
    "        df = df.rename(columns={\"typo\": \"query\", \"corrected\": \"correction\"})\n",
    "        \n",
    "        print(f\"Successfully fetched {len(df)} training examples from Supabase\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from Supabase: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch data from Supabase\n",
    "def retrain_model_and_upload(supabase_train_df):\n",
    "    \"\"\"Retrain the model and upload it to Supabase\"\"\"\n",
    "    # Initialize Supabase client\n",
    "    supabase_client = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)\n",
    "    \n",
    "    # Define paths\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    LOCAL_MODEL_PATH = \"./model/finetuned-embedding-model\"\n",
    "    OUTPUT_PATH = f\"./model/finetuned-embedding-model-{today}\"\n",
    "    ZIP_PATH = f\"./model/finetuned-embedding-model-{today}.zip\"\n",
    "    \n",
    "    try:\n",
    "        # Try to load the previous week's model for continued training\n",
    "        try:\n",
    "            print(\"Attempting to load last week's model...\")\n",
    "            model = load_model_from_supabase(supabase_client, days_ago=7)\n",
    "            print(f\"Successfully loaded model from last week: {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load last week's model: {e}. Using base model instead.\")\n",
    "            model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "        if not supabase_train_df.empty:\n",
    "            # Generate training examples\n",
    "            train_examples = prepare_training_examples(supabase_train_df)\n",
    "            print(f\"Created {len(train_examples)} training examples\")\n",
    "            \n",
    "            # Create data loader\n",
    "            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n",
    "            \n",
    "            # Define loss function\n",
    "            train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Training on: {device}\")\n",
    "            \n",
    "            # Fine-tune\n",
    "            print(f\"Starting fine-tuning for 5 epochs...\")\n",
    "            warmup_steps = int(len(train_dataloader) * 5 * 0.1)\n",
    "            \n",
    "            model.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                epochs=5,\n",
    "                optimizer_params={'lr': 1e-5},\n",
    "                warmup_steps=warmup_steps,\n",
    "                output_path=OUTPUT_PATH,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Model fine-tuned and saved to {OUTPUT_PATH}\")\n",
    "            \n",
    "            # Zip the model\n",
    "            zip_path = zip_model(OUTPUT_PATH, ZIP_PATH)\n",
    "            \n",
    "            # Upload to Supabase\n",
    "            model_url = upload_model_to_supabase(zip_path, supabase_client)\n",
    "            print(f\"Model uploaded successfully: {model_url}\")\n",
    "            \n",
    "            return model, model_url\n",
    "        else:\n",
    "            print(\"No training data available. Model not trained.\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training and upload: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Call the function to retrain and upload\n",
    "supabase_train_df = fetch_training_data_from_supabase()\n",
    "model, model_url = retrain_model_and_upload(supabase_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815ad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n",
      "Starting additional fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Projects\\vehicle-insurance-backend-api\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to ./model/finetuned-embedding-model-v2\n"
     ]
    }
   ],
   "source": [
    "# Create data loader for the new examples\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n",
    "\n",
    "# Define loss function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Continue fine-tuning\n",
    "print(f\"Starting additional fine-tuning for 5 epochs...\")\n",
    "warmup_steps = int(len(train_dataloader) * 5 * 0.1)  # 10% of training as warmup\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=5,  # Fewer epochs for continued training\n",
    "    optimizer_params={'lr': 1e-5},\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Model fine-tuned and saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a688c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'supabase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msupabase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_client\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'supabase'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from supabase import create_client\n",
    "import zipfile\n",
    "import requests\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"/path/to/local/model\",\n",
    "    repo_id=\"ShawnSean/AutoValidate-Embedding-Model\",\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "\n",
    "# Add the parent directory to sys.path so 'app' can be imported\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Import required functions from your notebook\n",
    "from app.api.services.config import SUPABASE_ANON_KEY, SUPABASE_URL\n",
    "\n",
    "\n",
    "def upload_model_to_supabase_direct_api(zip_path, bucket_name=\"codenection-sss-model-training\"):\n",
    "    \"\"\"\n",
    "    Upload a zipped model to Supabase storage using direct API calls and chunked upload\n",
    "    \n",
    "    Args:\n",
    "        zip_path: Path to the zipped model file\n",
    "        bucket_name: Name of the storage bucket\n",
    "    \n",
    "    Returns:\n",
    "        URL of the uploaded file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "    \n",
    "    # Get the file size\n",
    "    file_size = os.path.getsize(zip_path)\n",
    "    print(f\"File size: {file_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # Get the filename\n",
    "    file_name = os.path.basename(zip_path)\n",
    "    storage_path = f\"model/{file_name}\"\n",
    "    SUPABASE_SERVICE_KEY = os.getenv(\"SUPABASE_SERVICE_KEY\") \n",
    "    print(f\"Supabase Service Key\")\n",
    "    if not SUPABASE_SERVICE_KEY:\n",
    "        print(\"SUPABASE_SERVICE_KEY environment variable not found.\")\n",
    "        print(\"Please set it to your service role key from Supabase dashboard.\")\n",
    "        return None\n",
    "    \n",
    "    supabase_client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)\n",
    "    # Supabase API URL and headers\n",
    "    api_url = f\"{SUPABASE_URL}/storage/v1/object/{bucket_name}/{storage_path}\"\n",
    "    headers = {\n",
    "        \"apikey\": SUPABASE_ANON_KEY,\n",
    "        \"Authorization\": f\"Bearer {SUPABASE_SERVICE_KEY}\",\n",
    "        \"Content-Type\": \"application/octet-stream\",\n",
    "        \"x-upsert\": \"true\"  # Overwrite if exists\n",
    "    }\n",
    "    \n",
    "    # Upload in chunks to avoid memory issues\n",
    "    CHUNK_SIZE = 5 * 1024 * 1024  # 5MB chunks\n",
    "    \n",
    "    # Upload with requests using chunked transfer\n",
    "    with open(zip_path, 'rb') as f:\n",
    "        # Create a generator that yields chunks of the file\n",
    "        def read_in_chunks():\n",
    "            while True:\n",
    "                chunk = f.read(CHUNK_SIZE)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "        \n",
    "        print(f\"Starting upload of {file_name} to {bucket_name}/{storage_path}\")\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            data=read_in_chunks()  # Use a generator to read and send chunks\n",
    "        )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Upload successful with status code: {response.status_code}\")\n",
    "        public_url = f\"{SUPABASE_URL}/storage/v1/object/public/{bucket_name}/{storage_path}\"\n",
    "        print(f\"Public URL: {public_url}\")\n",
    "        return public_url\n",
    "    else:\n",
    "        print(f\"Upload failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        raise Exception(f\"Failed to upload file: {response.text}\")\n",
    "\n",
    "def upload_initial_model():\n",
    "    \"\"\"Upload the existing fine-tuned model to Supabase storage\"\"\"\n",
    "    \n",
    "    # Path to your existing model\n",
    "    model_path = \"./model/finetuned-embedding-model\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model directory not found at {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create a zip filename with current date\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    zip_path = f\"./model/finetuned-embedding-model-{today}.zip\"\n",
    "    \n",
    "    print(f\"1. Zipping model from {model_path}...\")\n",
    "    zip_path = zip_model_in_chunks(model_path, zip_path)\n",
    "    \n",
    "    print(f\"2. Uploading model to Supabase...\")\n",
    "    model_url = upload_model_to_supabase_direct_api(zip_path)\n",
    "    \n",
    "    print(f\"3. Model uploaded successfully!\")\n",
    "    print(f\"   URL: {model_url}\")\n",
    "    \n",
    "    return model_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting initial model upload...\")\n",
    "    try:\n",
    "        upload_initial_model()\n",
    "        print(\"Upload process completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19988746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "def zip_model_in_chunks(model_path, output_zip=None):\n",
    "    \"\"\"\n",
    "    Zip a model directory into a compressed file\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model directory\n",
    "        output_zip: Path for the output zip file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created zip file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
    "    \n",
    "    # Create a zip filename with current date if not provided\n",
    "    if output_zip is None:\n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        output_zip = f\"model_{today}.zip\"\n",
    "    \n",
    "    # Create the zip file with smaller compression for better memory usage\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_STORED) as zipf:\n",
    "        # Walk through all files in the directory\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                # Calculate path for file in zip\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(model_path))\n",
    "                \n",
    "                # Add file to zip\n",
    "                zipf.write(file_path, arcname=arcname)\n",
    "    \n",
    "    print(f\"Model zipped successfully to {output_zip}\")\n",
    "    return output_zip\n",
    "\n",
    "\n",
    "def upload_model_to_hf(zip_path, repo_id=None, repo_type=\"model\"):\n",
    "    \"\"\"\n",
    "    Upload a single file (zip) to Hugging Face Hub.\n",
    "    Requires HF_TOKEN in env and HF_REPO (or pass repo_id).\n",
    "    Returns public URL to the uploaded file in the repo.\n",
    "    \"\"\"\n",
    "    from huggingface_hub import HfApi\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    if not HF_TOKEN:\n",
    "        print(\"HF_TOKEN not set in environment.\")\n",
    "        return None\n",
    "\n",
    "    # repo id from env if not provided\n",
    "    repo_id = repo_id or os.getenv(\"HF_REPO\")\n",
    "    if not repo_id:\n",
    "        print(\"HF_REPO not set. Set env HF_REPO like 'username/repo-name' or pass repo_id.\")\n",
    "        return None\n",
    "\n",
    "    file_name = os.path.basename(zip_path)\n",
    "    path_in_repo = f\"models/{file_name}\"  # where file will live in the repo\n",
    "\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "    print(f\"Uploading {zip_path} -> {repo_id}:{path_in_repo} ...\")\n",
    "\n",
    "    # upload_file will stream the file and does not require loading whole file into memory\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=zip_path,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=repo_type,\n",
    "        token=HF_TOKEN,\n",
    "        commit_message=f\"Upload model {file_name} ({datetime.now().isoformat()})\"\n",
    "    )\n",
    "\n",
    "    url = f\"https://huggingface.co/{repo_id}/resolve/main/{path_in_repo}\"\n",
    "    print(f\"Upload complete. URL: {url}\")\n",
    "    return url\n",
    "\n",
    "def upload_initial_model():\n",
    "    \"\"\"\n",
    "    Zip local model and upload to Hugging Face Hub\n",
    "    \"\"\"\n",
    "    # Path to your existing model\n",
    "    model_path = \"./model/finetuned-embedding-model\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model directory not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "    # Create a zip filename with current date\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    zip_path = f\"./model/finetuned-embedding-model-{today}.zip\"\n",
    "\n",
    "    print(f\"1. Zipping model from {model_path}...\")\n",
    "    zip_path = zip_model_in_chunks(model_path, zip_path)  # re-uses zip (no chunking while uploading)\n",
    "\n",
    "    print(f\"2. Uploading model to Hugging Face Hub...\")\n",
    "    model_url = upload_model_to_hf(zip_path)\n",
    "    if model_url:\n",
    "        print(f\"3. Model uploaded successfully!\\n   URL: {model_url}\")\n",
    "    else:\n",
    "        print(\"Upload failed.\")\n",
    "    return model_url\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Starting initial model upload...\")\n",
    "#     try:\n",
    "#         upload_initial_model()\n",
    "#         print(\"Upload process completed.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7e66d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest model: models/finetuned-embedding-model-20250925.zip (date: 20250925)\n",
      "Downloading model from Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model to ./latest_model\\20250925...\n",
      "Loading model from ./latest_model\\20250925\\finetuned-embedding-model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from huggingface_hub import snapshot_download, HfApi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_model_from_hf(\n",
    "    repo_id=\"ShawnSean/AutoValidate-Embedding-Model\", \n",
    "    model_date=None, \n",
    "    use_temp=True,\n",
    "    cache_dir=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Download and load a model from Hugging Face Hub\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Hugging Face repo ID (default: \"ShawnSean/AutoValidate-Embedding-Model\")\n",
    "        model_date: Specific date of model to load (format: YYYYMMDD, default: latest)\n",
    "        use_temp: Whether to use a temporary directory (deleted on program exit)\n",
    "        cache_dir: Custom cache directory (if use_temp=False)\n",
    "        \n",
    "    Returns:\n",
    "        Loaded SentenceTransformer model\n",
    "    \"\"\"\n",
    "    # Set up HF_TOKEN from env if not already in environment\n",
    "    if \"HF_TOKEN\" not in os.environ:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "    \n",
    "    # Disable progress bars for cleaner output\n",
    "    os.environ[\"HF_HUB_DISABLE_PROGRESS\"] = \"1\"\n",
    "    \n",
    "    # Get all model files in the repo to find the latest if no date specified\n",
    "    api = HfApi()\n",
    "    \n",
    "    try:\n",
    "        # List model files in the repo\n",
    "        model_files = [\n",
    "            f for f in api.list_repo_files(repo_id=repo_id)\n",
    "            if f.startswith(\"models/finetuned-embedding-model-\") and f.endswith(\".zip\")\n",
    "        ]\n",
    "        \n",
    "        if not model_files:\n",
    "            raise FileNotFoundError(f\"No model files found in {repo_id}\")\n",
    "        \n",
    "        # Extract the date from filename\n",
    "        date_from_file = None\n",
    "        \n",
    "        if model_date is None:\n",
    "            # Find the latest model if no specific date requested\n",
    "            # Sort by date in filename\n",
    "            model_files.sort(reverse=True)\n",
    "            model_file = model_files[0]\n",
    "            # Extract date from filename (e.g., finetuned-embedding-model-20250925.zip -> 20250925)\n",
    "            date_from_file = model_file.split('-')[-1].split('.')[0]\n",
    "            print(f\"Using latest model: {model_file} (date: {date_from_file})\")\n",
    "        else:\n",
    "            # Find the model with the specified date\n",
    "            date_str = str(model_date)\n",
    "            matching_files = [f for f in model_files if date_str in f]\n",
    "            if not matching_files:\n",
    "                raise FileNotFoundError(f\"No model found for date {model_date}\")\n",
    "            model_file = matching_files[0]\n",
    "            date_from_file = date_str\n",
    "            print(f\"Using model from {model_date}: {model_file}\")\n",
    "        \n",
    "        # Create extract directory with date\n",
    "        base_dir = cache_dir if not use_temp else \"./latest_model\"\n",
    "        extract_dir = os.path.join(base_dir, date_from_file)\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the final model directory path\n",
    "        model_dir = os.path.join(extract_dir, \"finetuned-embedding-model\")\n",
    "        \n",
    "        # Skip download if model already exists\n",
    "        if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
    "            print(f\"Model already exists at {model_dir}, skipping download\")\n",
    "        else:\n",
    "            # Download the specific file from the repo\n",
    "            print(f\"Downloading model from Hugging Face Hub...\")\n",
    "            repo_dir = snapshot_download(\n",
    "                repo_id=repo_id,\n",
    "                allow_patterns=[model_file],\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            \n",
    "            # Path to the downloaded zip file\n",
    "            zip_path = os.path.join(repo_dir, model_file)\n",
    "            \n",
    "            # Extract the model\n",
    "            print(f\"Extracting model to {extract_dir}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "            \n",
    "            # Clean up the download cache to save space\n",
    "            if os.path.exists(repo_dir) and repo_dir != extract_dir:\n",
    "                shutil.rmtree(repo_dir)\n",
    "        \n",
    "        # Load the model with SentenceTransformer\n",
    "        print(f\"Loading model from {model_dir}...\")\n",
    "        model = SentenceTransformer(model_dir)\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from Hugging Face: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the latest model\n",
    "    model = load_model_from_hf()\n",
    "    \n",
    "    # Or load a specific date's model\n",
    "    # model = load_model_from_hf(model_date=\"20250925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ded14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
